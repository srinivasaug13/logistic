---
title: "Ensemble Methods"
author: "Granger Huntress"
date: "4/16/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ensemble Methods

We are going to now try out our ensemble methods we discussed in the last lecture. We will try Bagging, Boosting, SMOTE and k-Fold Cross Validation and see what transpires.

To begin, we will load some German Credit data.


```{r}

rm(list=ls())
set.seed(248)
#the bigger dataset using only numbers
setwd("~/Documents/2019/ML")
germanData <- read.csv("creditcard.csv", header=T)

#let's see what the outcome Class data looks like
table(germanData$Class)
```

```{r}

library(caTools)        #really just using this for the sample.split command
split <- sample.split(germanData$Class, SplitRatio = 0.90)
gd <- subset(germanData, split == FALSE)


split <- sample.split(gd$Class, SplitRatio = 0.75)


gd_train <- subset(gd, split == TRUE)
gd_test <- subset(gd, split == FALSE)

#we can check the ratios of the minority class to majority class
table(gd_train$Class)

table(gd_test$Class)
```

#logistic regression
```{r}

german_logistic <- glm(Class~., data=gd_train, family=binomial(link="logit"))

gd_test$log.pred<-predict(german_logistic, gd_test[1:30], type="response")

table(gd_test$Class,gd_test$log.pred>0.5)

```


#knn
```{r}
#knn compare
library(class)
knn_fit<- knn(train = gd_train[,1:30], test = gd_test[,1:30], cl= gd_train[,31],k = 3,prob=TRUE) 

table(gd_test[,31],knn_fit)

```

#naive bayes
```{r}
library(e1071)

nb_gd<-naiveBayes(x=gd_train[,1:30], y=as.factor(gd_train[,31]))

pred_nb<-predict(nb_gd,newdata = gd_test[,1:30])

table(gd_test[,31],pred_nb)
```



```{r}

#loading a few libraries

library(gbm)          # basic implementation using AdaBoost
library(xgboost)      # a faster implementation of a gbm
library(caret)        # an aggregator package for performing many machine learning models


```

## Bagging

Let's start using bagging




```{r}
library(ipred)
library(rpart)

#we can modify the maxdepth and minsplit if needed
#r doc, https://www.rdocumentation.org/packages/ipred/versions/0.4-0/topics/bagging
German.bagging <- bagging(Class ~.,
                           data=gd_train,
                           control=rpart.control(maxdepth=5, minsplit=4))


gd_test$pred.class <- predict(German.bagging, gd_test)


#gd_test$pred.class<- ifelse(gd_test$pred.class<0.5,0,1)


#confusionMatrix(data=factor(gd_test$pred.class),
#                reference=factor(gd_test$Class),
#                positive='1')

table(gd_test$Class,gd_test$pred.class>0.5)
```


#Boosting

Now let's try some general boosting techniques.

```{r}

gbm.fit <- gbm(
  formula = Class ~ .,
  distribution = "bernoulli",
  data = gd_train,
  n.trees = 10000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

gd_test$pred.class <- predict(gbm.fit, gd_test, type = "response")

table(gd_test$Class,gd_test$pred.class>0.5)

```


```{r}

# XGBoost works with matrices that contain all numeric variables
# we also need to split the training data and label

gd_features_train<-as.matrix(gd_train[,1:30])
gd_label_train<-as.matrix(gd_train[,31])
gd_features_test<-as.matrix(gd_test[,1:30])

xgb.fit <- xgboost(
  data = gd_features_train,
  label = gd_label_train,
  eta = 0.001,
  max_depth = 3,
  min_child_weight = 3,
  nrounds = 10000,
  nfold = 5,
  objective = "binary:logistic",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

#gd_features_test<-as.matrix(gd_features_test[,1:ncol(gd_features_test)-1])

gd_test$xgb.pred.class <- predict(xgb.fit, gd_features_test)

table(gd_test$Class,gd_test$xgb.pred.class>0.5)

#or simply the total correct of the minority class
sum(gd_test$Class==1 & gd_test$xgb.pred.class>=0.5)
```

```{r}
#let's play with shrinkage, known as eta in xbg
tp_xgb<-vector()
lr <- c(0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1)
md<-c(1,3,5,7,9,15)
nr<-c(2, 50, 100, 1000, 10000)
for (i in md) {

xgb.fit <- xgboost(
  data = gd_features_train,
  label = gd_label_train,
  eta = 0.7,
  max_depth = 5,
  nrounds = 50,
  nfold = 5,
  objective = "binary:logistic",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

gd_test$xgb.pred.class <- predict(xgb.fit, gd_features_test)

tp_xgb<-cbind(tp_xgb,sum(gd_test$Class==1 & gd_test$xgb.pred.class>=0.5))

}

tp_xgb
```


```{r}
#now we put them all into our best fit!

xgb.fit <- xgboost(
  data = gd_features_train,
  label = gd_label_train,
  eta = 0.7,
  max_depth = 5,
  nrounds = 50,
  nfold = 5,
  objective = "binary:logistic",  # for regression models
  verbose = 1,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

gd_test$xgb.pred.class <- predict(xgb.fit, gd_features_test)

sum(gd_test$Class==1 & gd_test$xgb.pred.class>=0.5)


```


```{r}

#working with SMOTE
library(DMwR)

table(gd$Class)

smote.train<-subset(gd, split == TRUE)
smote.test<-subset(gd, split == FALSE)

smote.train$Class<-as.factor(smote.train$Class)
balanced.gd <- SMOTE(Class ~., smote.train, perc.over = 4800, k = 5, perc.under = 1000)

```

```{r}
#now put our SMOTE data into our best xgboost

smote_features_train<-as.matrix(balanced.gd[,1:30])
smote_label_train<-as.matrix(balanced.gd$Class)

smote.xgb.fit <- xgboost(
  data = smote_features_train,
  label = smote_label_train,
  eta = 0.7,
  max_depth = 5,
  nrounds = 50,
  nfold = 5,
  objective = "binary:logistic",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

smote_features_test<-as.matrix(smote.test[,1:30])
smote.test$smote.pred.class <- predict(smote.xgb.fit, smote_features_test)

table(smote.test$Class,smote.test$smote.pred.class>=0.5)

sum(gd_test$Class==1 & gd_test$xgb.pred.class>=0.5)


```


